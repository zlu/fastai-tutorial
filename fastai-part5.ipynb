{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "For Chinese, where the concept of \"word\" does not exist, fastai's `subword` support via `SentencePiece` becomes crucial.  As there are no whitespace between words:\n",
    "```\n",
    "我喜欢学习 （I like studying)\n",
    "```\n",
    "SentencePiece will segment it into:\n",
    "```\n",
    "我/喜欢/学习/\n",
    "```\n",
    "This is done via a unsupervised learning process directly on raw Chinese text, which allows the modelt o genralize to new words  and expressions.  This is important for Chinese because it allows the model to learn the correct word boundaries and avoid splitting words in unexpected ways.  As a result, we could apply them to AWD-LSTM or Transformer-based models using in fastai's NLP pipeline.  \n",
    "Here's another example:\n",
    "```\n",
    "我喜欢吃辣椒。\n",
    "▁我 ▁喜欢 ▁吃 ▁辣 ▁椒 ▁。\n",
    "```\n",
    "Each token is either a character or a frequent combination, which the model has learned overtime from the train data (raw text).  The `_` marks the beginnings of a new subword.  As you can see, it has corrected grouped `喜` (happiness) and `欢` (joy) as a single token `喜欢` (like).\n",
    "\n",
    "Specifically, subword tokenization can be done like this:\n",
    "\n",
    "```python\n",
    "# Suppose `text` is the raw text\n",
    "def subword(vocab_size):\n",
    "  sw = SubwordTokenizer(vocab_size=vocab_size)\n",
    "  sw.setup(text)\n",
    "  return ' '.join(first(sw([text])))\n",
    "```\n",
    "\n",
    "Note, depending on the size of the vocabulary, the subword tokenization will like yield different results.  The larger the vocabulary, the less tokens per sentence, the faster the training time, but also a larger embedding matrix.  This is why we need to find a balance.\n",
    "\n",
    "## Numericalization - Turning Tokens into Numbers.\n",
    "\n",
    "We have in part 2 of this series learned how to turn images into numbers for categorization tasks.  Same principle applies to text.  Computer works only with numbers, so we need to turn tokens from the previous step into numbers. Then we can feed them into a neural network.\n",
    "\n",
    "We will leverage fastai's `Numericalize()` to transform the tokens into integers.  This is done by creating a `Vocab` object, which is a mapping of tokens to integers.  The `Numericalize()` will then use this mapping to transform the tokens into integers.  Then we can feed them into a fastai `Datasets` object, which applies the same transformation to the whole dataset.  the resulting `dataset.items` will contain the integers.\n",
    "\n",
    "Here's a toy example:\n",
    "\n",
    "```python\n",
    "from fastai.text.all import *\n",
    "from fastcore.basics import noop\n",
    "\n",
    "tokens = [[\"I\", \"love\", \"deep\", \"learning\"], [\"Fastai\", \"makes\", \"it\", \"simple\"]]\n",
    "\n",
    "# Apply noop + numericalize\n",
    "dsets = Datasets(tokens, [[noop, Numericalize()]])\n",
    "\n",
    "# Show vocab\n",
    "vocab = dsets[0][0].vocab\n",
    "print(\"Vocab:\\n\", vocab)\n",
    "\n",
    "# Show numericalized data\n",
    "for i, item in enumerate(dsets):\n",
    "    print(f\"Sentence {i+1}: {item[0]}\")\n",
    "```\n",
    "\n",
    "And let's visualize what's happening:\n",
    "\n",
    "```python\n",
    "Tokenized Text → [ \"Fastai\", \"makes\", \"it\", \"simple\" ]\n",
    "                   ↓\n",
    "               Numericalize\n",
    "                   ↓\n",
    "Integer IDs    → [ 6, 7, 8, 9 ]\n",
    "\n",
    "Where:\n",
    "    vocab = { \"Fastai\": 6, \"makes\": 7, \"it\": 8, \"simple\": 9, ... }\n",
    "```\n",
    "\n",
    "## DataLoader Creation\n",
    "\n",
    "We have seen `DataLoader` in the previous parts of this series.  It take raw or processed data (such as numericalized text from the previous step) and turn it into batches.  This is important for training a neural network because it allows the model to see the data in a batch-wise manner, which is more efficient and stable.  There are two important concepts here to understand:\n",
    "\n",
    "1. Batching\n",
    "\n",
    "Neural networks train faster and more reliable with **batches** of data.  So the output from the previous step:\n",
    "```python\n",
    "[2, 3, 4, 5]  # \"I love deep learning\"\n",
    "[6, 7, 8, 9]  # \"Fastai makes it simple\"\n",
    "```\n",
    "are grouped into a batch.\n",
    "\n",
    "2. Padding\n",
    "\n",
    "As sequences datastructure are often of variable-length and tensors **must** be of the same size (rectangular-shaped) to fit into GPU memory, we need to pad the sequences to the same length.  For example, the two variable-sequences:\n",
    "```\n",
    "Original:         [2, 3, 4, 5]\n",
    "                  [6, 7]\n",
    "After padding:    [2, 3, 4, 5]\n",
    "                  [6, 7, 0, 0]\n",
    "```                  \n",
    "will be padded to the same length.\n",
    "\n",
    "Suppose we have already created a `TextDataLoaders` previous like this:\n",
    "\n",
    "```python\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
    "```\n",
    "\n",
    "Then we can create a padded batch like this:\n",
    "\n",
    "```python\n",
    "x, y = dls.one_batch()\n",
    "print(type(x))  # torch.Tensor\n",
    "print(x.shape)  # e.g., torch.Size([64, 72]) — 64 examples, each 72 tokens long\n",
    "```\n",
    "\n",
    "## Language Model Fine-tuning\n",
    "\n",
    "Just like what we did in the previous part of this series, we can fine-tune a language model as such:\n",
    "\n",
    "```python\n",
    "# Create a language model learner\n",
    "learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "\n",
    "# Fine-tune on your corpus\n",
    "learn.fine_tune(1)\n",
    "```\n",
    "\n",
    "`fine_tune(1)` runs one epoch of training on the learn object.  Here the pretrained base model is `frozen`, meaning its weights are not updated during training.  This is important because we want to focus only on the `head`, or the newly added classification layers), trained.  This allows the model to adapt its final layers to the new task without disrupting the pre-trainned representations.  Then the entire model is `unfrozen` (including the base model) and _all_ layers are fine-tuned together.  When we pass `1` to `fine_tune`, fastai will run 1 epock with the base model frozen but skip the unfrozen step.  So we will need to pass in a number greater than 1 to fine-tune the entire model.\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "Finally perform the inferencing:\n",
    "```python\n",
    "learn.predict(\"I love deep learning\")\n",
    "```\n",
    "\n",
    "This completes our journey of applying RNNs to text classification.  In the next part of this series, we will dive deeper into how to build a RNN network from scratch!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
