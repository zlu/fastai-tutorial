{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with fastai\n",
    "\n",
    "In the previous tutorial we have seen how to leverage pretrained model and fine tune it to perform categorization tasks on images (MNIST).  The principle of transfer learning is applied there can also be applied to NLP tasks.  In this tutorial, we will use a pre-trained model calld AWD_LSTM to classify Chinese movie reviews.  AWD_LSTM is a variant of LSTM.  LSTM is a type of recurrent neural network (RNN) that is designed to handle long sequences of text.  We will leave detailed discussion on RNNs to a later tutorial.\n",
    "\n",
    "## Chinese NLP with fastai: Practical Example\n",
    "\n",
    "Chinese language processing is a challenging task because majority of NLP models are trained with western languages such as English.  Unlike English, Chinese does not use spaces to separate words.  This makes tokenization more challenging. Lukily there are libraries like jieba for Chinese tokenization.  Jieba and pkuseg are two libraries designed to handle Chinese segmentation effectively.  Pre-trained word embeddings such as Word2Vec, Glove, or FastText can be used as long as they are trained on Chinese corpora. Towards the end of this guide, I will show you how to use Google's BERT variant, Chinese BERT, to capture the context in Chinese text.  XLM-RoBERTa is another multilingual model that performs well on Chinese text.  Besides Chinese BERT, there are many local-grown models such as ERNIE (Enhanced Representation through kNowledge IntEgration) and PaddleNLP from Baidu, FastBERT and AliceMind from Alibaba, and last but not least, TecentPretrain and Chinese Word Vectors from Tencent.\n",
    "\n",
    "## The Process\n",
    "Largely speaking, there are two basic blocks to NLP tasks: text preprocessing and text classification.\n",
    "\n",
    "In text preprocessing, we want to prepare the text in such a way that computer is able to interpret it.  Interpretation of the contextual meaning of the texts turns out to be a non-trivial task even for RNNs.  The introduction of transformer and self-attention had made a breakthrough in this area (hence the transformer example at the end).  For the sake of simplicity, we will mainly focus now on **tokenization** and **word embeddings** steps.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the action of converting text into \"tokens\", which could be characters (\"a\", \"b\", \"c\", ...) or words (\"hello\", \"world\", ...), or even substrings depending on the granularity of the model.  This is where Chinese language gets interesting, as unlike English or alphabet-based languages, even Chinese character (我，喜，欢，爱，中) carry meanings of their own!  Word segmentation in Chinese thus becomes a tougher task as unlike English, words are separated by spaces, Chinese people had to learn how to spot word boundaries by reading and memorization.  Special algorithms are thus needed to segment Chinese text.  In addition, foreign words, numbers, and symbols in Chinese texts require special handling.\n",
    "\n",
    "### Word Embeddings\n",
    "Word embeddings are a way to represent words as vectors.  In the last tutorial, we saw how to convert MNIST dataset (grayscale images) into 3D vectors (height, width, color).  We will do something here quite similar conceptually.  What's special about these vectors is that they are learned from a large corpus of text and those similar in meanings are close to each other in a high-dimensional vector space (物以类聚).  In this tutorial, we will use create a custom fastai `DataBlock`, `ChineseTextBlock`, to tokenize and embed Chinese text.\n",
    "\n",
    "### Text Classification\n",
    "Text classification is the task of assigning a label to a piece of text.  For example, we can classify a movie review as positive or negative.  We  will use fastai's dataloaders and `AWD_LSTM` to build a text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install fastai jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Chinese Text\n",
    "\n",
    "For demonstration purposes, we'll create a small dataset of Chinese movie reviews. In a real application, you would load your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bcc8904e-e1eb-4257-90c4-23cb3fc313ed",
       "rows": [
        [
         "0",
         "这是一部让人回味无穷的佳作，值得一看。",
         "positive"
        ],
        [
         "1",
         "剧情紧凑，特效惊人，是今年最好看的电影之一。",
         "positive"
        ],
        [
         "2",
         "导演的手法很独特，将故事讲述得引人入胜。",
         "positive"
        ],
        [
         "3",
         "特效做得很差，剧情漏洞百出，非常失望。",
         "negative"
        ],
        [
         "4",
         "音乐配乐恰到好处，为电影增添了不少气氛。",
         "positive"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>这是一部让人回味无穷的佳作，值得一看。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>剧情紧凑，特效惊人，是今年最好看的电影之一。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>导演的手法很独特，将故事讲述得引人入胜。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>特效做得很差，剧情漏洞百出，非常失望。</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>音乐配乐恰到好处，为电影增添了不少气氛。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text     label\n",
       "0     这是一部让人回味无穷的佳作，值得一看。  positive\n",
       "1  剧情紧凑，特效惊人，是今年最好看的电影之一。  positive\n",
       "2    导演的手法很独特，将故事讲述得引人入胜。  positive\n",
       "3     特效做得很差，剧情漏洞百出，非常失望。  negative\n",
       "4    音乐配乐恰到好处，为电影增添了不少气氛。  positive"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample positive and negative movie reviews in Chinese\n",
    "positive_reviews = [\n",
    "    \"这部电影非常精彩，演员的表演令人印象深刻。\",\n",
    "    \"剧情紧凑，特效惊人，是今年最好看的电影之一。\",\n",
    "    \"导演的手法很独特，将故事讲述得引人入胜。\",\n",
    "    \"音乐配乐恰到好处，为电影增添了不少气氛。\",\n",
    "    \"这是一部让人回味无穷的佳作，值得一看。\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"情节拖沓，演员表演生硬，浪费了我的时间。\",\n",
    "    \"特效做得很差，剧情漏洞百出，非常失望。\",\n",
    "    \"导演似乎不知道自己想要表达什么，整部电影混乱不堪。\",\n",
    "    \"对白尴尬，角色塑造单薄，完全不推荐。\",\n",
    "    \"这部电影毫无亮点，是我今年看过最差的一部。\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "reviews = positive_reviews + negative_reviews\n",
    "labels = ['positive'] * len(positive_reviews) + ['negative'] * len(negative_reviews)\n",
    "\n",
    "df = pd.DataFrame({'text': reviews, 'label': labels})\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Text Tokenization\n",
    "\n",
    "Let's explore different tokenization methods for Chinese text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word-level Tokenization with Jieba\n",
    "\n",
    "We need to distinguish the meaning of `word` in the context of NLP for Chinese.  A Chinese word is composed of Chinese characters that provide meaning.  For example, the word `中国` is composed of two Chinese characters `中` and `国`.  The word `中国` has a different meaning than the word `中国` in `中国是一个伟大的国家` (the country China is a great country).  The word `中国` in the latter sentence is a noun phrase, while the word `中国` in the former sentence is a noun.  The word `中国` in the former sentence is a single word, while the word `中国` in the latter sentence is two words.  The word `中国` in the former sentence is a single token, while the word `中国` in the latter sentence is two tokens. \n",
    "In English however, a word is a a word like `China`.  So a Chinese NLP word is really conceptually more similar to a `subword` in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['这部', '电影', '非常', '精彩', '，', '演员', '的', '表演', '令人', '印象', '深刻', '。']\n",
      "Number of tokens: 12\n"
     ]
    }
   ],
   "source": [
    "def chinese_word_tokenizer(text):\n",
    "    \"\"\"Tokenize Chinese text using Jieba word segmentation\"\"\"\n",
    "    # Handle Path objects by reading the file\n",
    "    if hasattr(text, 'read_text'):\n",
    "        text = text.read_text(encoding='utf-8')\n",
    "    elif hasattr(text, 'open'):\n",
    "        text = text.open(encoding='utf-8').read()\n",
    "    \n",
    "    # Convert to lowercase if there's any English text mixed in\n",
    "    text = str(text).lower()\n",
    "    # Use Jieba to segment words\n",
    "    words = jieba.cut(text)\n",
    "    return list(words)\n",
    "\n",
    "# Example\n",
    "sample_text = \"这部电影非常精彩，演员的表演令人印象深刻。\"\n",
    "word_tokens = chinese_word_tokenizer(sample_text)\n",
    "print(f\"Word tokens: {word_tokens}\")\n",
    "print(f\"Number of tokens: {len(word_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Character-level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character tokens: ['这', '部', '电', '影', '非', '常', '精', '彩', '，', '演', '员', '的', '表', '演', '令', '人', '印', '象', '深', '刻', '。']\n",
      "Number of tokens: 21\n"
     ]
    }
   ],
   "source": [
    "def chinese_char_tokenizer(text):\n",
    "    \"\"\"Tokenize Chinese text at character level\"\"\"\n",
    "    # Handle Path objects by reading the file\n",
    "    if hasattr(text, 'read_text'):\n",
    "        text = text.read_text(encoding='utf-8')\n",
    "    elif hasattr(text, 'open'):\n",
    "        text = text.open(encoding='utf-8').read()\n",
    "    \n",
    "    # Convert to string if it's not already\n",
    "    text = str(text)\n",
    "    # Remove spaces if any\n",
    "    text = text.replace(\" \", \"\")\n",
    "    # Split into characters\n",
    "    return list(text)\n",
    "\n",
    "# Example\n",
    "char_tokens = chinese_char_tokenizer(sample_text)\n",
    "print(f\"Character tokens: {char_tokens}\")\n",
    "print(f\"Number of tokens: {len(char_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom fastai Tokenizer for Chinese\n",
    "\n",
    "In fastai's NLP framework, special tokens play a crucial role in helping models understand text structure.  In the code snippet below, you will see `xxbos', which tells the model that a new sentence is starting. Some other often used special tokens include `xxmaj` (for capitalization), `xxup` (for uppercase), `xxmaj` (for uppercase), `xxrep` (for repeating a word), and `xxwrep` (for repeating a word with a space in between).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizer:\n",
      "['xxbos', '这部', '电影', '非常', '精彩', '，', '演员', '的', '表演', '令人', '印象', '深刻', '。']\n",
      "\n",
      "Character tokenizer:\n",
      "['xxbos', '这', '部', '电', '影', '非', '常', '精', '彩', '，', '演', '员', '的', '表', '演', '令', '人', '印', '象', '深', '刻', '。']\n"
     ]
    }
   ],
   "source": [
    "# Define a string class that can be truncated for display\n",
    "class TitledStr(str):\n",
    "    \"\"\"A string that can be truncated for display purposes\"\"\"\n",
    "    def truncate(self, n):\n",
    "        return TitledStr(self[:n] + '...' if len(self) > n else self)\n",
    "        \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Display text in the context\"\n",
    "        return show_text(self, ctx=ctx, **kwargs)\n",
    "        \n",
    "def show_text(text, ctx=None, **kwargs):\n",
    "    \"Helper function to display text\"\n",
    "    if ctx is None: ctx = {'text': text}\n",
    "    else: ctx['text'] = text\n",
    "    return ctx\n",
    "\n",
    "class ChineseTokenizer(Transform):\n",
    "    def __init__(self, tokenizer_func=chinese_word_tokenizer):\n",
    "        self.tokenizer_func = tokenizer_func\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        tokens = self.tokenizer_func(x)\n",
    "        # Add special tokens like BOS (beginning of sentence)\n",
    "        tokens = ['xxbos'] + tokens\n",
    "        return tokens\n",
    "    \n",
    "    def decodes(self, x):\n",
    "        text = ''.join(x) if isinstance(x[0], str) and len(x[0]) == 1 else ' '.join(x)\n",
    "        # Create a text object with a truncate method\n",
    "        return TitledStr(text)\n",
    "\n",
    "# Create instances for both tokenization methods\n",
    "word_tokenizer = ChineseTokenizer(chinese_word_tokenizer)\n",
    "char_tokenizer = ChineseTokenizer(chinese_char_tokenizer)\n",
    "\n",
    "# Example\n",
    "print(\"Word tokenizer:\")\n",
    "print(word_tokenizer.encodes(sample_text))\n",
    "print(\"\\nCharacter tokenizer:\")\n",
    "print(char_tokenizer.encodes(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our sample data to disk for fastai to read\n",
    "# In a real application, you would use your own dataset\n",
    "\n",
    "# Create directories\n",
    "path = Path('chinese_reviews')\n",
    "path.mkdir(exist_ok=True)\n",
    "(path/'train').mkdir(exist_ok=True)\n",
    "(path/'test').mkdir(exist_ok=True)\n",
    "(path/'train'/'positive').mkdir(exist_ok=True)\n",
    "(path/'train'/'negative').mkdir(exist_ok=True)\n",
    "(path/'test'/'positive').mkdir(exist_ok=True)\n",
    "(path/'test'/'negative').mkdir(exist_ok=True)\n",
    "\n",
    "# Split data into train and test\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save files\n",
    "for i, row in train_df.iterrows():\n",
    "    with open(path/'train'/row['label']/f\"{i}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(row['text'])\n",
    "        \n",
    "for i, row in test_df.iterrows():\n",
    "    with open(path/'test'/row['label']/f\"{i}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Custom TextBlock for Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom TextBlock for Chinese\n",
    "class ChineseTextBlock(TextBlock):\n",
    "    @delegates(TextBlock.__init__)\n",
    "    def __init__(self, tokenizer_func=chinese_word_tokenizer, vocab=None, is_lm=False, seq_len=72, **kwargs):\n",
    "        # Create the tokenizer transform\n",
    "        tok_tfm = ChineseTokenizer(tokenizer_func)\n",
    "        # Pass the tokenizer to the parent class\n",
    "        super().__init__(tok_tfm=tok_tfm, vocab=vocab, is_lm=is_lm, seq_len=seq_len, **kwargs)\n",
    "        self.tokenizer = tok_tfm\n",
    "    \n",
    "    def get_tokenizer(self, **kwargs):\n",
    "        return self.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos 剧情 xxunk ， 特效 xxunk ， 是 今年 xxunk xxunk 的 电影 xxunk 。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos 导演 xxunk 不 xxunk xxunk xxunk xxunk xxunk ， xxunk 电影 xxunk xxunk 。</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos 剧情 xxunk ， 特效 xxunk ， 是 今年 xxunk xxunk 的 电影 xxunk 。</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos 导演 xxunk 不 xxunk xxunk xxunk xxunk xxunk ， xxunk 电影 xxunk xxunk 。</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataLoaders for classification\n",
    "chinese_block = ChineseTextBlock(tokenizer_func=chinese_word_tokenizer, is_lm=False)\n",
    "\n",
    "dls = DataBlock(\n",
    "    blocks=(chinese_block, CategoryBlock),\n",
    "    get_items=get_text_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, bs=4)  # Small batch size for our small dataset\n",
    "\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Chinese Text Classifier\n",
    "\n",
    "For demonstration purposes, we'll build a simple classifier. In a real application with more data, you would follow the ULMFiT approach with language model pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.740130</td>\n",
       "      <td>0.703697</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.786076</td>\n",
       "      <td>0.737464</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.756238</td>\n",
       "      <td>0.728642</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.760603</td>\n",
       "      <td>0.852913</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>0.872675</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.740408</td>\n",
       "      <td>0.778970</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.748301</td>\n",
       "      <td>0.836783</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.760259</td>\n",
       "      <td>0.835310</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.743047</td>\n",
       "      <td>0.804637</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.820280</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a simple text classifier\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "\n",
    "# Train for a few epochs (with our tiny dataset, this is just for demonstration)\n",
    "learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 0.5288\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to predict on new text\n",
    "def predict_chinese_text(learner, text):\n",
    "    \"\"\"Helper function to predict sentiment on new Chinese text\"\"\"\n",
    "    # Create a temporary file with the text\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    try:\n",
    "        # Use the file path for prediction (which fastai can handle)\n",
    "        pred_class, pred_idx, probs = learner.predict(Path(temp_path))\n",
    "        return pred_class, pred_idx, probs\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "\n",
    "# Test on a new review\n",
    "new_review = \"这部电影情节紧凑，演员演技精湛，非常推荐！\"\n",
    "pred_class, pred_idx, probs = predict_chinese_text(learn, new_review)\n",
    "print(f\"Prediction: {pred_class}\")\n",
    "print(f\"Probability: {probs[pred_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Word vs. Character Tokenization\n",
    "\n",
    "Let's compare the performance of word-level vs. character-level tokenization for Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.789537</td>\n",
       "      <td>0.688798</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.885231</td>\n",
       "      <td>0.698306</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.906784</td>\n",
       "      <td>0.656616</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.794850</td>\n",
       "      <td>0.701583</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.793879</td>\n",
       "      <td>0.680373</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.811998</td>\n",
       "      <td>0.583346</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.869714</td>\n",
       "      <td>0.567899</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.895559</td>\n",
       "      <td>0.562752</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.891531</td>\n",
       "      <td>0.557563</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.883717</td>\n",
       "      <td>0.533585</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataLoaders with character-level tokenization\n",
    "char_block = ChineseTextBlock(tokenizer_func=chinese_char_tokenizer, is_lm=False)\n",
    "\n",
    "char_dls = DataBlock(\n",
    "    blocks=(char_block, CategoryBlock),\n",
    "    get_items=get_text_files,\n",
    "    get_y=parent_label,\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, bs=4)\n",
    "\n",
    "# Create a classifier with character-level tokenization\n",
    "char_learn = text_classifier_learner(char_dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "\n",
    "# Train for the same number of epochs\n",
    "char_learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level prediction: positive\n",
      "Word-level probability: 0.5288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level prediction: negative\n",
      "Character-level probability: 0.5670\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions\n",
    "new_review = \"这部电影情节紧凑，演员演技精湛，非常推荐！\"\n",
    "\n",
    "# Word-level prediction\n",
    "word_pred_class, word_pred_idx, word_probs = predict_chinese_text(learn, new_review)\n",
    "print(f\"Word-level prediction: {word_pred_class}\")\n",
    "print(f\"Word-level probability: {word_probs[word_pred_idx]:.4f}\")\n",
    "\n",
    "# Character-level prediction\n",
    "char_pred_class, char_pred_idx, char_probs = predict_chinese_text(char_learn, new_review)\n",
    "print(f\"Character-level prediction: {char_pred_class}\")\n",
    "print(f\"Character-level probability: {char_probs[char_pred_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Chinese Models (Advanced)\n",
    "\n",
    "For production applications, you would typically use pre-trained models. Here's how you might integrate a pre-trained Chinese BERT model using the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you have the transformers library installed\n",
    "# !pip install transformers\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load pre-trained Chinese BERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
    "\n",
    "# # Tokenize a sample text\n",
    "# inputs = tokenizer(new_review, return_tensors=\"pt\")\n",
    "\n",
    "# # Get predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "#     predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#     print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to adapt fastai's NLP capabilities for Chinese text processing. We've explored:\n",
    "\n",
    "1. Different tokenization methods for Chinese (word-level vs. character-level)\n",
    "2. Creating custom tokenizers for fastai\n",
    "3. Building a simple Chinese text classifier\n",
    "4. Comparing different approaches\n",
    "\n",
    "For real-world applications with larger datasets, you would follow the complete ULMFiT approach:\n",
    "1. Pre-train a language model on a large Chinese corpus\n",
    "2. Fine-tune the language model on your domain-specific data\n",
    "3. Fine-tune a classifier using the language model\n",
    "\n",
    "You would also likely use more advanced models like Chinese BERT, RoBERTa, or MacBERT for state-of-the-art performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
